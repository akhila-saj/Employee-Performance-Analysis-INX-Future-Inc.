{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Employee Performance Analysis (INX Future Inc.) \n",
    "\n",
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Libraries\n",
    "- Import Dataset\n",
    "- Separation of Predictors and Target\n",
    "- Split data into Train and Test\n",
    "- Balance the dataset\n",
    "- Hyperparameter Tuning\n",
    "- Models\n",
    "  - Random Forest\n",
    "  - XGBoost\n",
    "  - Decision Tree\n",
    "  - Evaluation \n",
    "- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing processed dataset for modeling\n",
    "df = pd.read_excel('Employees_ProcessedDataset.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['EmpDepartment_Sales', 'EmpDepartment_Development',\n",
       "        'EmpEnvironmentSatisfaction', 'EmpJobLevel', 'EmpLastSalaryHikePercent',\n",
       "        'EmpWorkLifeBalance', 'ExperienceYearsAtThisCompany',\n",
       "        'ExperienceYearsInCurrentRole', 'YearsSinceLastPromotion',\n",
       "        'YearsWithCurrManager', 'PerformanceRating'],\n",
       "       dtype='object'),\n",
       " (1200, 11))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the columns and shape of dataset\n",
    "df.columns , df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate the Data into Predictors and Target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate the dataset into predictors and target variable.\n",
    "X = df.drop(['PerformanceRating'], axis=1)\n",
    "y = df.PerformanceRating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data (X,y) into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the target variable and predictors into train and test.\n",
    "X_train , X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing Train data (SMOTE-Synthetic Minority Oversampling Technique)\n",
    "SMOTE is a statistical technique for increasing the number of cases in your dataset in a balanced way. The module works by generating new instances from existing minority cases that you supply as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    656\n",
       "3    656\n",
       "2    656\n",
       "Name: PerformanceRating, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Balancing Dataset based on the categorical values using SMOTE Oversampling Method.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees.It is also one of the most used algorithms, because of its simplicity and diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using GridSearch CV\n",
    "Hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [12, 14, 15],\n",
       "                         'min_samples_leaf': [2, 3, 4, 5],\n",
       "                         'min_samples_split': [2, 3, 4, 5],\n",
       "                         'n_estimators': [50, 100, 150]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hyperparameter Tuning with GridSearch CV\n",
    "\n",
    "params = {'n_estimators':[50,100,150],\n",
    "    'max_depth': [12,14,15],\n",
    "    'min_samples_split':[2,3,4,5],\n",
    "     'min_samples_leaf':[2,3,4,5]}\n",
    "\n",
    "grid_model = GridSearchCV(RandomForestClassifier(), params)\n",
    "grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 14, 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 50}\n",
      "0.9583472184549413\n"
     ]
    }
   ],
   "source": [
    "### Best parameters and score after Hyperparameter Tuning\n",
    "print(grid_model.best_params_)\n",
    "print(grid_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=3,\n",
       "                       n_estimators=50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building Random Forest Classifier model\n",
    "model_ran = RandomForestClassifier(\n",
    "                      n_estimators=50,\n",
    "                      max_depth=5,\n",
    "                      min_samples_leaf=2, min_samples_split= 3)\n",
    "\n",
    "# fit the model\n",
    "model_ran.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy Score : 96% \n",
      "\n",
      "----------------------------------------\n",
      "Confusion Matrix: \n",
      " \n",
      " col_0               2    3   4\n",
      "PerformanceRating             \n",
      "2                  47    4   0\n",
      "3                   2  216   0\n",
      "4                   1    6  24\n",
      "----------------------------------------\n",
      "Classification Report: \n",
      " \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           2       0.94      0.92      0.93        51\n",
      "           3       0.96      0.99      0.97       218\n",
      "           4       1.00      0.77      0.87        31\n",
      "\n",
      "    accuracy                           0.96       300\n",
      "   macro avg       0.97      0.90      0.93       300\n",
      "weighted avg       0.96      0.96      0.96       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test data:\n",
    "\n",
    "y_pred_ran = model_ran.predict(X_test)\n",
    "\n",
    "print('Random Forest Accuracy Score :',\"{0:.0%}\".format(accuracy_score(y_test,y_pred_ran)), '\\n')\n",
    "print('----------------------------------------')\n",
    "print('Confusion Matrix:','\\n','\\n',pd.crosstab(y_test,y_pred_ran))\n",
    "print('----------------------------------------')\n",
    "print('Classification Report:','\\n','\\n',classification_report(y_test,y_pred_ran))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "XGBoost is a scalable and accurate implementation of gradient boosting machines and it has proven to push the limits of computing power for boosted trees algorithms as it was built and developed for the sole purpose of model performance and computational speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Tuning with GridSearch CV\n",
    "\n",
    "params_xg = {'n_estimators':[50,100,150],\n",
    "          'max_depth': [12,14,15],\n",
    "          'min_child_weight':[3,4,5],\n",
    "         'learning_rate':[0.1,0.2,0.5]}\n",
    "\n",
    "grid_model_xg = GridSearchCV(XGBClassifier(), params_xg)\n",
    "grid_model_xg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.5, 'max_depth': 12, 'min_child_weight': 3, 'n_estimators': 50}\n",
      "0.9532710763229615\n"
     ]
    }
   ],
   "source": [
    "### Best parameters and score after Hyperparameter Tuning\n",
    "print(grid_model_xg.best_params_)\n",
    "print(grid_model_xg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:05:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.6, max_delta_step=0, max_depth=14,\n",
       "              min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=10, n_jobs=4, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building XGBClassifier model:\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_xg = XGBClassifier(learning_rate=0.6, max_depth=14, min_child_weight=5, n_estimators=10)\n",
    "\n",
    "## Fit the model\n",
    "model_xg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier Accuracy Score : 95% \n",
      "\n",
      "----------------------------------------\n",
      "Confusion Matrix: \n",
      " \n",
      " col_0               2    3   4\n",
      "PerformanceRating             \n",
      "2                  46    5   0\n",
      "3                   2  213   3\n",
      "4                   0    6  25\n",
      "----------------------------------------\n",
      "Classification Report: \n",
      " \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           2       0.96      0.90      0.93        51\n",
      "           3       0.95      0.98      0.96       218\n",
      "           4       0.89      0.81      0.85        31\n",
      "\n",
      "    accuracy                           0.95       300\n",
      "   macro avg       0.93      0.90      0.91       300\n",
      "weighted avg       0.95      0.95      0.95       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test data:\n",
    "\n",
    "y_pred_xg = model_xg.predict(X_test)\n",
    "\n",
    "print('XGBClassifier Accuracy Score :',\"{0:.0%}\".format(accuracy_score(y_test,y_pred_xg)), '\\n')\n",
    "print('----------------------------------------')\n",
    "print('Confusion Matrix:','\\n','\\n',pd.crosstab(y_test,y_pred_xg))\n",
    "print('----------------------------------------')\n",
    "print('Classification Report:','\\n','\\n',classification_report(y_test,y_pred_xg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'max_depth': [12, 14, 15], 'max_features': [8, 9, 10],\n",
       "                         'min_samples_leaf': [2, 3, 4, 5],\n",
       "                         'min_samples_split': [2, 3, 4]})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hyperparameter Tuning with GridSearch CV\n",
    "\n",
    "params_tr = {\n",
    "    'max_depth': [12,14,15],\n",
    "    'min_samples_split':[2,3,4],\n",
    "    'min_samples_leaf':[2,3,4,5],\n",
    "    'max_features':[8,9,10]\n",
    "                      }\n",
    "\n",
    "grid_model_tr = GridSearchCV(DecisionTreeClassifier(), params_tr)\n",
    "grid_model_tr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 14, 'max_features': 8, 'min_samples_leaf': 5, 'min_samples_split': 3}\n",
      "0.9144444444444444\n"
     ]
    }
   ],
   "source": [
    "### Best parameters and score after Hyperparameter Tuning\n",
    "print(grid_model_tr.best_params_)\n",
    "print(grid_model_tr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, max_features=9, min_samples_leaf=2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building XGBClassifier model:\n",
    "\n",
    "model_tr = DecisionTreeClassifier(max_depth=10,\n",
    "                                 max_features=9,\n",
    "                                  min_samples_leaf=2,\n",
    "                                  min_samples_split=2)\n",
    "\n",
    "model_tr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier Accuracy Score : 94% \n",
      "\n",
      "----------------------------------------\n",
      "Confusion Matrix: \n",
      " \n",
      " col_0               2    3   4\n",
      "PerformanceRating             \n",
      "2                  47    4   0\n",
      "3                   5  212   1\n",
      "4                   1    6  24\n",
      "----------------------------------------\n",
      "Classification Report: \n",
      " \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           2       0.89      0.92      0.90        51\n",
      "           3       0.95      0.97      0.96       218\n",
      "           4       0.96      0.77      0.86        31\n",
      "\n",
      "    accuracy                           0.94       300\n",
      "   macro avg       0.93      0.89      0.91       300\n",
      "weighted avg       0.94      0.94      0.94       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test data:\n",
    "\n",
    "y_pred_tr = model_tr.predict(X_test)\n",
    "\n",
    "print('DecisionTreeClassifier Accuracy Score :',\"{0:.0%}\".format(accuracy_score(y_test,y_pred_tr)), '\\n')\n",
    "print('----------------------------------------')\n",
    "print('Confusion Matrix:','\\n','\\n',pd.crosstab(y_test,y_pred_tr))\n",
    "print('----------------------------------------')\n",
    "print('Classification Report:','\\n','\\n',classification_report(y_test,y_pred_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Random Forest Classifier with Hyperparameter tuning GridSearch CV gives the accuracy score 96%  with F1 score of 2,3 and 4 is   93%, 97% and 87% respectively. This model performed well compared to other models. \n",
    "\n",
    "- XGBoost Classifier with Hyperparameter tuning gives the accuracy score of 95%  with F1 score of 2,3 and 4 as 93%, 96% and 85% respectively. This model performed well compared to Decision Tree.\n",
    "\n",
    "- DecisionTreeClassifier with Hyperparameter tuning gives the accuracy score of 94%  with F1 score of 2,3 and 4 as 90%, 96% and 86% respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
